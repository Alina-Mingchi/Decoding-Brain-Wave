'''GoogLeNet'''import pywtimport numpy as npimport matplotlib.pyplot as pltfrom scipy.io import loadmatfrom scipy.io import savematimport xlrd    from PIL import Imagefrom torchvision import transformsimport scaleogram as scgimport torchimport torch.optim as optimfrom torch.autograd import Variableimport torch.nn.functional as Fimport itertoolsimport cv2from numpy import loadtxt# Load labelslabeltrain = loadtxt('labeltrain.csv', delimiter=',')labeltest = loadtxt('labeltest.csv', delimiter=',')# Change the last three layers of the model, and set those three layers requires_grad to be true# For other layers switch off the requires_grad in order to freeze those layersmodel = torch.hub.load('pytorch/vision:v0.6.0', 'googlenet', pretrained=True)model.eval()for param in model.parameters():  param.requires_grad = Falsemodel.dropout = torch.nn.Dropout(p=0.5,inplace=False)model.fc = torch.nn.Linear(in_features=1024,out_features=2,bias=True)model.eval()# Check which parameters are not freezed# for param in model.parameters():#   print(param)   out_temp = []valout_temp = []# Read training datafor i in range(95):    filename = 'Train_scalo/'+str(i)+'.jpg'    input_img = Image.open(filename)    # Rescale the data to be 3x224x224    process_data = transforms.Compose([        transforms.Resize((325,325),interpolation=Image.NEAREST),        transforms.CenterCrop(224),        transforms.ToTensor(),    ])    input_tensor = process_data(input_img)    input_batch = input_tensor.unsqueeze(0)        temp_plot = input_tensor.numpy()    temp_p = temp_plot.reshape((224,224,3))    plt.imshow(temp_p)    out_temp.append(input_batch)        result = torch.cat(out_temp, dim=0)# Prepare the input to the model as [batch,3,224,224]# Read testing/validation datafor ii in range(23):    filename = 'Test_scalo/'+str(ii)+'.jpg'    input_img = Image.open(filename)    process_data = transforms.Compose([        transforms.Resize((325,325),interpolation=Image.NEAREST),        transforms.CenterCrop(224),        transforms.ToTensor(),    ])    input_tensor = process_data(input_img)    input_batch = input_tensor.unsqueeze(0)        temp_plot = input_tensor.numpy()    temp_p = temp_plot.reshape((224,224,3))    plt.imshow(temp_p)    valout_temp.append(input_batch)        valresult = torch.cat(valout_temp, dim=0)# Prepare the input to the model as [batch,3,224,224]# %%# Show the prediction without trainingwith torch.no_grad():    output = model(result)    temp_class = torch.nn.functional.softmax(output, dim=0)    predict = temp_class[:,0] < temp_class[:,1]    predict = predict * 1    print('Before Training')    print(predict)    vloss = np.zeros((0))tloss = np.zeros((0))loss_function = torch.nn.CrossEntropyLoss()optimizer = optim.SGD(model.parameters(),lr = 0.0001,momentum = 0.8)# Train the modelfor epoch in range(100):    print(epoch)                optimizer.zero_grad()    output= model.train()(result)        out_s = F.softmax(output)    out_arr = out_s.detach().numpy()    predict = out_arr[:,0] < out_arr[:,1]    predict = predict * 1        # Calculate and store training loss in array tloss    labeltrain = torch.LongTensor(labeltrain)    loss = loss_function(output,labeltrain)    tloss = np.append(tloss,loss.detach().numpy())#    print('Training')#    print(loss)    loss.backward()    optimizer.step()        valoutput = model.eval()(valresult)    out_s = F.softmax(valoutput)    out_arr = out_s.detach().numpy()    valpredict = out_arr[:,0] < out_arr[:,1]    valpredict = valpredict * 1#    print('Validation')#    print(valpredict)    # Calculate and store validation loss in array vloss        labeltest = torch.LongTensor(labeltest)    valloss = loss_function(valoutput,labeltest)    vloss = np.append(vloss,valloss.detach().numpy())#    print('Validation')#    print(valloss)print('Training loss',tloss)print('Validation loss',vloss)# Show the prediction after training the modelwith torch.no_grad():    output = model(result)    out_s = F.softmax(output)    out_arr = out_s.detach().numpy()    predict = out_arr[:,0] < out_arr[:,1]    predict = predict * 1    print('After Training')    print(predict)        output1 = model(valresult)    out_s1 = F.softmax(output1)    out_arr1 = out_s1.detach().numpy()    predict1 = out_arr1[:,0] < out_arr1[:,1]    predict1 = predict1 * 1    print('After Training, the predicted lable on validation set is')    print(predict1)# %% Plot of loss     plt.plot(tloss,label = "Training Loss")plt.plot(vloss,label = "Validation Loss")plt.xlabel('Epoch')plt.ylabel('Loss')plt.title('Loss over epoch Channel No.30')plt.legend()plt.show()