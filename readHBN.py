'''Read HBN filesIt takes 24 mintues to load all data for one channel.'''import torchimport torch.optim as optimimport torch.nn.functional as Ffrom torch.autograd import Variableimport itertoolsfrom sklearn.metrics import confusion_matrixfrom scipy.io import loadmatfrom scipy.io import savematimport xlrd    import pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport os# %% Data loadingarr = np.zeros((118,500000))count = 0dir = pd.read_csv('../MDD_indi.csv', sep=',',header=None,dtype=str)dir = np.array(dir)for i in dir:    print(i+'RestingState_data.csv')    aaa = i+'RestingState_data.csv'    data = pd.read_csv(aaa[0], nrows=1) #only read one row    len = data.shape    len = len[1]    arr[count,0:len] = data    count += 1    print(count)small = arr[:,1000:21000]# plot the recording of one channel for each participant# for k in range(118):#     plt.plot(small[k,:])#     plt.show()# save the preprocessed data to avoid taking the above steps in each run# np.savetxt('hbn_useddata.csv', small, delimiter=',')# temp = pd.read_csv('hbn_useddata.csv', sep=',',header=None)# small = np.array(small)# assign labels to each participants (row)data_label = np.zeros((118,20001))data_label[0:42,0] = 1data_label[42:,0] = 0data_label[:,1:] = smallnp.random.shuffle(data_label)# separate training and testing data, and training and testing labelstrain = data_label[0:95,1:]test = data_label[95:119,1:]labeltrain = data_label[0:95,0]labeltest = data_label[95:119,0]t1 = train[0:19,:]t2 = train[19:38,:]t3 = train[38:57,:]t4 = train[57:76,:]t5 = train[76:95,:]t1 = t1.Tt2 = t2.Tt3 = t3.Tt4 = t4.Tt5 = t5.Tlt1 = labeltrain[0:19]lt2 = labeltrain[19:38]lt3 = labeltrain[38:57]lt4 = labeltrain[57:76]lt5 = labeltrain[76:95]# %% Set up the LSTM modelend_point = 10input_size = 20000hidden_size = 50batch_size = 1n_class = 2drop = 0.2# uncomment when investigating number of layerslayerarr = np.array([1,2,3,4])for num in range(4):    num_layers = layerarr[num]    print('Number of layer is ',num_layers)# uncomment when investigating on dropout rate# num_layers = 3 %change to the optimal number according to the investigation on numger of layers# droparr = np.array([0.1,0.2,0.5])# for count in range(3):#     drop = droparr[count]#     print('Dropout rate is', drop)            def ToVariable(x):        tmp = torch.FloatTensor(x)        return Variable(tmp)        #Build the LSTM model for classification    class LSTMcla(torch.nn.Module):                def __init__(self,input_size,hidden_dim,n_class,batch_size,drop,num_layers):            super(LSTMcla,self).__init__()            self.input_size = input_size            self.hidden_dim = hidden_dim            self.n_class = n_class            self.batch_size = batch_size            self.drop = drop            self.num_layers = num_layers            self.lstm = torch.nn.LSTM(input_size,hidden_dim,num_layers)            self.drop_layer = torch.nn.Dropout(drop)            self.linear_layer = torch.nn.Linear(batch_size * hidden_dim,n_class)                        self.hidden = self.init_hidden()                                def init_hidden(self):            return (Variable(torch.zeros(num_layers,self.batch_size,self.hidden_dim)),                    Variable(torch.zeros(num_layers,self.batch_size,self.hidden_dim)))                    def forward(self,seq):            #structure of the LSTM with LSTM cell, then Dropout layer, and then Fully connected layer, here named as linear_layer            lstm_out, self.hidden = self.lstm(seq.view(len(seq),self.batch_size,-1),self.hidden)            output = self.drop_layer(lstm_out[:,-1])            output = self.linear_layer(output)            return output                        model = LSTMcla(input_size,hidden_size,n_class,batch_size,drop,num_layers)    loss_function = torch.nn.CrossEntropyLoss()    optimizer = optim.SGD(model.parameters(),lr = 0.015,momentum = 0.8)            # %Separate data to 5 folds    # For single channel processing    seq1 = np.concatenate((t1,t2,t3,t4),axis = 1)    label1 = np.concatenate((lt1,lt2,lt3,lt4))    val1 = t5    labelval1 = lt5        seq2 = np.concatenate((t1,t2,t3,t5),axis = 1)    label2 = np.concatenate((lt1,lt2,lt3,lt5))    val2 = t4    labelval2 = lt4        seq3 = np.concatenate((t1,t2,t5,t4),axis = 1)    label3 = np.concatenate((lt1,lt2,lt5,lt4))    val3 = t3    labelval3 = lt3        seq4 = np.concatenate((t1,t5,t3,t4),axis = 1)    label4 = np.concatenate((lt1,lt5,lt3,lt4))    val4 = t2    labelval4 = lt2        seq5 = np.concatenate((t5,t2,t3,t4),axis = 1)    label5 = np.concatenate((lt5,lt2,lt3,lt4))    val5 = t1    labelval5 = lt1            # Temporary storage of different combinations of the folds as the input for training    seqarr = np.array([seq1,seq2,seq3,seq4,seq5])    labelarr = np.array([label1,label2,label3,label4,label5])    valarr = np.array([val1,val2,val3,val4,val5])    labelvalarr = np.array([labelval1,labelval2,labelval3,labelval4,labelval5])        tlossarr = np.zeros((5,200))    vlossarr = np.zeros((5,200))            for k in range(5):        print('Fold ',k)        seq = seqarr[k,:,:]        label = labelarr[k,:]        val = valarr[k,:,:]        labelval = labelvalarr[k,:]                        seq = seq.T        seq = torch.Tensor(seq)        label = torch.LongTensor(label)                val = val.T        val = torch.Tensor(val)        labelval = torch.LongTensor(labelval1)                # First disable gradient, and print out the initial prediction        with torch.no_grad():            output = model(seq)            out_s = F.softmax(output)            out_arr = out_s.detach().numpy()            predict = out_arr[:,0] > out_arr[:,1]            predict = predict * 1            predict = 1 - predict            print('Before Training')            print(predict)                vloss = np.zeros((0))        tloss = np.zeros((0))                # Train the model        for epoch in range(200):            # print(epoch)                                optimizer.zero_grad()            model.hidden = model.init_hidden()            output= model.train()(seq)            valoutput = model.eval()(val)                        loss = loss_function(output,label)            tloss = np.append(tloss,loss.detach().numpy())            # print('Training')        #    print(tloss)            loss.backward()            optimizer.step()                        out_s = F.softmax(valoutput)            out_arr = out_s.detach().numpy()            valpredict = out_arr[:,0] > out_arr[:,1]            valpredict = valpredict * 1            valpredict = 1 - valpredict        #    print('Validation')        #    print(valpredict)            valloss = loss_function(valoutput,labelval)            vloss = np.append(vloss,valloss.detach().numpy())        #    print('Validation')        #    print(vloss)                with torch.no_grad():            output = model(seq)            out_s = F.softmax(output)            out_arr = out_s.detach().numpy()            predict = out_arr[:,0] > out_arr[:,1]            predict = predict * 1            predict = 1 - predict            print('After Training')            print(predict)                # Plot of loss        plt.plot(tloss,label = 'Training')        plt.plot(vloss,label = 'Validation')        plt.xlabel('Epoch')        plt.ylabel('Cross Entropy Loss')        plt.title('Loss over epoch')        plt.rcParams.update({'font.size': 16})        plt.legend()        plt.show()                    tlossarr[k,:] = tloss        vlossarr[k,:] = vloss            avgtloss = np.mean(tlossarr,axis=0)    avgvloss = np.mean(vlossarr,axis=0)        plt.plot(avgtloss,label = 'Training Loss')    plt.plot(avgvloss,label = 'Validation Loss')    plt.rcParams.update({'font.size': 16})    plt.xlabel('Epoch')    plt.ylabel('Cross Entropy Loss')    plt.title('Loss over epoch')    plt.legend()    plt.show()        print('Minimum of validation loss is: ',np.min(avgvloss))    print('Optimal epoch is: ',np.argmin(avgvloss))# %% Train the model with optimal hyper-parameters for testingseq = trainlabel = labeltrainseq = seq.Tseq = torch.Tensor(seq)label = torch.LongTensor(label)model = LSTMcla(input_size,hidden_size,n_class,batch_size,drop,num_layers)loss_function = torch.nn.CrossEntropyLoss()optimizer = optim.SGD(model.parameters(),lr = 0.025,momentum = 0.8)tloss = []for epoch in range(np.argmin(avgvloss)+1):    print(epoch)        optimizer.zero_grad()    model.hidden = model.init_hidden()    output= model(seq)        loss = loss_function(output,label)    tloss = np.append(tloss,loss.detach().numpy())    print('Training')    print(tloss)    loss.backward()    optimizer.step()with torch.no_grad():    output = model(seq)    out_s = F.softmax(output)    out_arr = out_s.detach().numpy()    predict = out_arr[:,0] > out_arr[:,1]    predict = predict * 1    predict = 1 - predict    print('After Training')    print(predict)plt.plot(tloss,label = "Training")plt.xlabel('Epoch')plt.ylabel('Cross Entropy Loss')plt.title('Loss over epoch')plt.legend()plt.show()# %% Define plotting of confusion_matrix (from sklearn package)def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):    if normalize:        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]        print("Normalized confusion matrix")    else:        print('Confusion matrix, without normalization')    print(cm)    plt.imshow(cm, interpolation='nearest', cmap=cmap)    plt.title(title)    plt.colorbar()    tick_marks = np.arange(len(classes))    plt.xticks(tick_marks, classes, rotation=45)    plt.yticks(tick_marks, classes)    fmt = '.2f' if normalize else 'd'    thresh = cm.max() / 2.    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment="center", color="white" if cm[i, j] > thresh else "black")    plt.tight_layout()    plt.rcParams.update({'font.size': 26})    plt.ylabel('True label')    plt.xlabel('Predicted label')    plt.show()# %%##Testingseqq = testseqq = torch.Tensor(seqq)labeltest = torch.LongTensor(labeltest)with torch.no_grad():    testout = model.eval()(seqq)    testout_s = F.softmax(testout)    testout_arr = testout_s.detach().numpy()    tpredict = testout_arr[:,0] > testout_arr[:,1]    tpredict = tpredict * 1    tpredict = 1 - tpredict    print('Predict')    print(tpredict)#b = torch.LongTensor(labeltest)#print(loss_function(testout,b))cm = confusion_matrix(labeltest, tpredict)cmcategories = ('Healthy','Patient')plt.figure(figsize=(10,10))plot_confusion_matrix(cm, categories)#############################Reference########################### https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html# https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html# https://docs.scipy.org/doc/scipy/reference/tutorial/io.html